<html>
    <head>
        <title></title>
    </head>
    <body>
       <h1>Understanding the need for a system like Kubernetes</h1>
        <p>Before you start getting to know Kubernetes in detail, let’s take a quick look at how
            the development and deployment of applications has changed in recent years. This
            change is both a consequence of splitting big monolithic apps into smaller microservices 
            and of the changes in the infrastructure that runs those apps. Understanding these
            changes will help you better see the benefits of using Kubernetes and container tech-
            nologies such as Docker.
        </p>
        <ol>
            <li><h2>Moving from monolithic apps to microservices</h2></li>
            <p>Monolithic applications consist of components that are all tightly coupled together and
                have to be developed, deployed, and managed as one entity, because they all run as a sin-
                gle OS process. Changes to one part of the application require a redeployment of the
                whole application, and over time the lack of hard boundaries between the parts results
                in the increase of complexity and consequential deterioration of the quality of the whole
                system because of the unconstrained growth of inter-dependencies between these parts.
            </p>
            <p>Running a monolithic application usually requires a small number of powerful
                servers that can provide enough resources for running the application. To deal with
                increasing loads on the system, you then either have to vertically scale the servers (also
                known as scaling up) by adding more CPUs, memory, and other server components,
                or scale the whole system horizontally, by setting up additional servers and running
                multiple copies (or replicas) of an application (scaling out). While scaling up usually
                doesn’t require any changes to the app, it gets expensive relatively quickly and in prac-
                tice always has an upper limit. Scaling out, on the other hand, is relatively cheap hard-
                ware-wise, but may require big changes in the application code and isn’t always
                possible—certain parts of an application are extremely hard or next to impossible to
                scale horizontally (relational databases, for example). If any part of a monolithic
                application isn’t scalable, the whole application becomes unscalable, unless you can
                split up the monolith somehow.
            </p>
            <h3>SPLITTING APPS INTO MICROSERVICES</h3>
            <p>These and other problems have forced us to start splitting complex monolithic appli-
                cations into smaller independently deployable components called microservices. Each
                microservice runs as an independent process (see figure 1.1) and communicates with
                other microservices through simple, well-defined interfaces (APIs).
            </p>
            <figure>
                <img/ src="" alt="Image to be put" title="Monolithic s Microserices based app">
                <figcaption>Figure 1.1 Components inside a monolithic application vs. standalone microservices</figcaption>
            </figure>
            <p>Microservices communicate through synchronous protocols such as HTTP, over which
                they usually expose RESTful (REpresentational State Transfer) APIs, or through asyn-
                chronous protocols such as AMQP (Advanced Message Queueing Protocol). These
                protocols are simple, well understood by most developers, and not tied to any specific
                programming language. Each microservice can be written in the language that’s most
                appropriate for implementing that specific microservice.
            </p>
            <p>
                Because each microservice is a standalone process with a relatively static external
                API, it’s possible to develop and deploy each microservice separately. A change to one
                of them doesn’t require changes or redeployment of any other service, provided that
                the API doesn’t change or changes only in a backward-compatible way.
            </p>
            <h3>
                    SCALING MICROSERVICES
            </h3>
            <p>
            Scaling microservices, unlike monolithic systems, where you need to scale the system as
            a whole, is done on a per-service basis, which means you have the option of scaling only
            those services that require more resources, while leaving others at their original scale.
            Figure 1.2 shows an example. Certain components are replicated and run as multiple
            processes deployed on different servers, while others run as a single application process.
            When a monolithic application can’t be scaled out because one of its parts is unscal-
            able, splitting the app into microservices allows you to horizontally scale the parts that
            allow scaling out, and scale the parts that don’t, vertically instead of horizontally.
            </p>
            <figure>
                    <img/ src="" alt="Image to be put" title="Monolithic s Microserices based app">
                    <figcaption>Figure 1.2 Each microservice can be scaled individually.</figcaption>
            </figure>
            <h3>
                    DEPLOYING MICROSERVICES
            </h3>
            <p>
                As always, microservices also have drawbacks. When your system consists of only a
                small number of deployable components, managing those components is easy. It’s
                trivial to decide where to deploy each component, because there aren’t that many
                choices. When the number of those components increases, deployment-related deci-
                sions become increasingly difficult because not only does the number of deployment
                combinations increase, but the number of inter-dependencies between the compo-
                nents increases by an even greater factor.
            </p>
            <p>
                Microservices perform their work together as a team, so they need to find and talk
                to each other. When deploying them, someone or something needs to configure all of
                them properly to enable them to work together as a single system. With increasing
                numbers of microservices, this becomes tedious and error-prone, especially when you
                consider what the ops/sysadmin teams need to do when a server fails.
            </p>
            <p>
                    Microservices also bring other problems, such as making it hard to debug and trace
                    execution calls, because they span multiple processes and machines. Luckily, these
                    problems are now being addressed with distributed tracing systems such as Zipkin.
            </p>
            <h3>
                    UNDERSTANDING THE DIVERGENCE OF ENVIRONMENT REQUIREMENTS
            </h3>
            <p>
                As I’ve already mentioned, components in a microservices architecture aren’t only
                deployed independently, but are also developed that way. Because of their indepen-
                dence and the fact that it’s common to have separate teams developing each compo-
                nent, nothing impedes each team from using different libraries and replacing them
                whenever the need arises. The divergence of dependencies between application com-
                ponents, like the one shown in figure 1.3, where applications require different ver-
                sions of the same libraries, is inevitable.
            </p>
            <figure>
                    <img/ src="" alt="Image to be put" title="Monolithic s Microserices based app">
                    <figcaption>Figure 1.3 Multiple applications running on the same host may have conflicting dependencies.</figcaption>
            </figure>
            <p>
                Deploying dynamically linked applications that require different versions of shared
                libraries, and/or require other environment specifics, can quickly become a night-
                mare for the ops team who deploys and manages them on production servers. The
                bigger the number of components you need to deploy on the same host, the harder it
                will be to manage all their dependencies to satisfy all their requirements.
            </p>
            <li><h2>Providing a consistent environment to applications</h2></li>
            <p>
                Regardless of how many individual components you’re developing and deploying,
                one of the biggest problems that developers and operations teams always have to deal
                with is the differences in the environments they run their apps in. Not only is there a
                huge difference between development and production environments, differences
                even exist between individual production machines. Another unavoidable fact is that
                the environment of a single production machine will change over time.
            </p>
            <p>
                These differences range from hardware to the operating system to the libraries
                that are available on each machine. Production environments are managed by the
                operations team, while developers often take care of their development laptops on
                their own. The difference is how much these two groups of people know about sys-
                tem administration, and this understandably leads to relatively big differences
                between those two systems, not to mention that system administrators give much more
                emphasis on keeping the system up to date with the latest security patches, while a lot
                of developers don’t care about that as much.
            </p>
            <p>
                Also, production systems can run applications from multiple developers or devel-
                opment teams, which isn’t necessarily true for developers’ computers. A production
                system must provide the proper environment to all applications it hosts, even though
                they may require different, even conflicting, versions of libraries.
            </p>
            <p>
                To reduce the number of problems that only show up in production, it would be
                ideal if applications could run in the exact same environment during development
                and in production so they have the exact same operating system, libraries, system con-
                figuration, networking environment, and everything else. You also don’t want this
                environment to change too much over time, if at all. Also, if possible, you want the
                ability to add applications to the same server without affecting any of the existing
                applications on that server.
            </p>
            <li><h2>Moving to continuous delivery: DevOps and NoOps</h2></li>
            <p>
                In the last few years, we’ve also seen a shift in the whole application development pro-
                cess and how applications are taken care of in production. In the past, the develop-
                ment team’s job was to create the application and hand it off to the operations team,
                who then deployed it, tended to it, and kept it running. But now, organizations are
                realizing it’s better to have the same team that develops the application also take part
                in deploying it and taking care of it over its whole lifetime. This means the developer,
                QA, and operations teams now need to collaborate throughout the whole process.
                This practice is called DevOps.
            </p>
            <h3>
                    UNDERSTANDING THE BENEFITS
            </h3>
            <p>
                    Having the developers more involved in running the application in production leads
                    to them having a better understanding of both the users’ needs and issues and the
                    problems faced by the ops team while maintaining the app. Application developers
                    are now also much more inclined to give users the app earlier and then use their feed-
                    back to steer further development of the app.
            </p>
            <p>
                    To release newer versions of applications more often, you need to streamline the
                    deployment process. Ideally, you want developers to deploy the applications them-
                    selves without having to wait for the ops people. But deploying an application often
                    requires an understanding of the underlying infrastructure and the organization of
                    the hardware in the datacenter. Developers don’t always know those details and, most
                    of the time, don’t even want to know about them.
            </p>
            <h3>
                    LETTING DEVELOPERS AND SYSADMINS DO WHAT THEY DO BEST
            </h3>
            <p>
                    Even though developers and system administrators both work toward achieving the
                    same goal of running a successful software application as a service to its customers, they
                    have different individual goals and motivating factors. Developers love creating new fea-
                    tures and improving the user experience. They don’t normally want to be the ones mak-
                    ing sure that the underlying operating system is up to date with all the security patches
                    and things like that. They prefer to leave that up to the system administrators.
            </p>
            <p>
                    The ops team is in charge of the production deployments and the hardware infra-
                    structure they run on. They care about system security, utilization, and other aspects
                    that aren’t a high priority for developers. The ops people don’t want to deal with the
                    implicit interdependencies of all the application components and don’t want to think
                    about how changes to either the underlying operating system or the infrastructure
                    can affect the operation of the application as a whole, but they must.
            </p>
            <p>
                    Ideally, you want the developers to deploy applications themselves without know-
                    ing anything about the hardware infrastructure and without dealing with the ops
                    team. This is referred to as NoOps. Obviously, you still need someone to take care of
                    the hardware infrastructure, but ideally, without having to deal with peculiarities of
                    each application running on it.
            </p>
            <p>
                    As you’ll see, Kubernetes enables us to achieve all of this. By abstracting away the
                    actual hardware and exposing it as a single platform for deploying and running apps,
                    it allows developers to configure and deploy their applications without any help from
                    the sysadmins and allows the sysadmins to focus on keeping the underlying infrastruc-
                    ture up and running, while not having to know anything about the actual applications
                    running on top of it.
            </p>
        </ol>
    </body>
</html>